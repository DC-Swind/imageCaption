{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8888/notebooks/Train_model_step3.ipynb#模型训练\" data-toc-modified-id=\"模型训练-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>模型训练</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Train_model_step3.ipynb#导入工具包，并选择GPU卡\" data-toc-modified-id=\"导入工具包，并选择GPU卡-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>导入工具包，并选择GPU卡</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Train_model_step3.ipynb#默认参数设置\" data-toc-modified-id=\"默认参数设置-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>默认参数设置</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Train_model_step3.ipynb#DataLoader初始化\" data-toc-modified-id=\"DataLoader初始化-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>DataLoader初始化</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Train_model_step3.ipynb#下面的代码就是训练中间的语言模型\" data-toc-modified-id=\"下面的代码就是训练中间的语言模型-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>下面的代码就是训练中间的语言模型</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练\n",
    "这是Image_Caption第三个运行的代码，目的不言而喻就是对模型进行训练,存储每次结果以及中间的模型,方便后续直接利用训练好的模型进行测试."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入工具包，并选择GPU卡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "import opts\n",
    "import models\n",
    "from dataloader import *\n",
    "import eval_utils\n",
    "import misc.utils as utils\n",
    "import jieba\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError:\n",
    "    print(\"Tensorflow not installed; No tensorboard logging.\")\n",
    "    tf = None\n",
    "\n",
    "def add_summary_value(writer, key, value, iteration):\n",
    "    summary = tf.Summary(value=[tf.Summary.Value(tag=key, simple_value=value)])\n",
    "    writer.add_summary(summary, iteration) \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 默认参数设置\n",
    "- input_json: 步骤二中得到的图片id与图片位置以及ix_to_word等信息.\n",
    "- caption_model:选用的训练模型,show_tell是老的模型,结果可以达到0.6,目前用的是top_down模型\n",
    "- output_json: 输出数据的位置\n",
    "- input_att_h5,input_fc_h5,input_label_h5: 步骤二中训练得到的图片的特征的位置\n",
    "- 其他的特征都和网络结构有关,不在解释."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Opt():\n",
    "    def __init__(self):\n",
    "        self.input_json = 'data/coco.json' \n",
    "        self.input_fc_h5 = 'data/coco_ai_challenger_talk_fc_.h5'\n",
    "    \n",
    "        self.input_att_h5 = 'data/coco_ai_challenger_talk_att_.h5'\n",
    "        self.input_label_h5 = 'data/coco_ai_challenger_talk_label_.h5'\n",
    "\n",
    "        self.start_from = None \n",
    "        self.caption_model = 'topdown'\n",
    "        self.rnn_size = 512\n",
    "        self.num_layers =  1\n",
    "        self.rnn_type = 'lstm'\n",
    "\n",
    "        self.input_encoding_size = 512\n",
    "        self.att_hid_size = 512\n",
    "        self.fc_feat_size = 2048\n",
    "\n",
    "        self.att_feat_size = 2048\n",
    "\n",
    "        self.max_epochs = 10\n",
    "        self.batch_size = 5\n",
    "        self.grad_clip = 0.1 \n",
    "\n",
    "        self.drop_prob_lm = 0.5\n",
    "        self.seq_per_img  = 5\n",
    "\n",
    "        self.beam_size = 5\n",
    "        self.optim = 'adam' \n",
    "        self.learning_rate  = 1e-5\n",
    "        self.learning_rate_decay_start  = -1\n",
    "\n",
    "        self.learning_rate_decay_every = 3\n",
    "        self.learning_rate_decay_rate = 0.8\n",
    "        self.optim_alpha = 0.9\n",
    "        self.optim_beta = 0.999\n",
    "        self.optim_epsilon = 1e-8\n",
    "        self.weight_decay = 0\n",
    "        self.id = 'st'\n",
    "        self.scheduled_sampling_start  = -1\n",
    "        self.scheduled_sampling_increase_every = 5\n",
    "        self.scheduled_sampling_max_prob  = 0.25\n",
    "\n",
    "        self.val_images_use = 3200\n",
    "        self.save_checkpoint_every = 2500\n",
    "        self.checkpoint_path = 'save'\n",
    "\n",
    "        self.language_eval = 0\n",
    "        self.losses_log_every = 25\n",
    "        self.load_best_score = 1\n",
    "\n",
    "        self.id = ''\n",
    "        self.train_only = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = Opt()\n",
    "opt.caption_model = 'topdown'\n",
    "opt.input_json = './preprocessed_data/coco_ai_challenger_talk.json'\n",
    "opt.input_fc_h5 = './preprocessed_data/coco_ai_challenger_talk_resnet151_fc_.h5'\n",
    "opt.input_att_h5 = './preprocessed_data/coco_ai_challenger_talk_resnet151_att_.h5'\n",
    "opt.input_label_h5 = './preprocessed_data/coco_ai_challenger_talk_resnet151_label_.h5'\n",
    " \n",
    "opt.batch_size  = 20\n",
    "opt.learning_rate  = 4e-4\n",
    "opt.learning_rate_decay_start = 0\n",
    "\n",
    "opt.scheduled_sampling_start  = 200\n",
    "opt.checkpoint_path  = './log_st'\n",
    "opt.save_checkpoint_every =  5000\n",
    "opt.val_images_use = 1000\n",
    "opt.max_epochs = 20\n",
    "opt.use_att  = utils.if_use_att(opt.caption_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader初始化\n",
    "直接参考DataLoader.py,就是简单的初始化.\n",
    "\n",
    "注意如果修改了前面预训练的大小,比如将之前开源代码的<font color = red>14换成了20,也可以自行改回去,不过改的时候注意把DataLoader和DataLoaderraw文件中的14或者20改回来(开源代码是写死的)</font>这边就需要在Dataloader里面将其换为20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(opt)\n",
    "opt.vocab_size = loader.vocab_size\n",
    "opt.seq_length = loader.seq_length\n",
    "tf_summary_writer = tf and tf.summary.FileWriter(opt.checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pictures_for_jupyter/att_size.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下面的代码就是训练中间的语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果之前有训练过,中间断了的,可以读取中间结果."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infos = {}\n",
    "histories = {}\n",
    "if opt.start_from is not None:\n",
    "    # open old infos and check if models are compatible\n",
    "    with open(os.path.join(opt.start_from, 'infos_'+opt.id+'.pkl')) as f:\n",
    "        infos = cPickle.load(f)\n",
    "        saved_model_opt = infos['opt']\n",
    "        need_be_same=[\"caption_model\", \"rnn_type\", \"rnn_size\", \"num_layers\"]\n",
    "        for checkme in need_be_same:\n",
    "            assert vars(saved_model_opt)[checkme] == vars(opt)[checkme], \"Command line argument and saved model disagree on '%s' \" % checkme\n",
    "\n",
    "    if os.path.isfile(os.path.join(opt.start_from, 'histories_'+opt.id+'.pkl')):\n",
    "        with open(os.path.join(opt.start_from, 'histories_'+opt.id+'.pkl')) as f:\n",
    "            histories = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_result_history = histories.get('val_result_history', {})\n",
    "loss_history = histories.get('loss_history', {})\n",
    "lr_history = histories.get('lr_history', {})\n",
    "ss_prob_history = histories.get('ss_prob_history', {})\n",
    "\n",
    "loader.iterators = infos.get('iterators', loader.iterators)\n",
    "loader.split_ix = infos.get('split_ix', loader.split_ix)\n",
    "if opt.load_best_score == 1:\n",
    "    best_val_score = infos.get('best_val_score', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = models.setup(opt)\n",
    "model = model.cuda() #should be \"model = model.cuda()\" \n",
    "update_lr_flag = True\n",
    "# Assure in training mode\n",
    "model.train()\n",
    "\n",
    "crit = utils.LanguageModelCriterion()\n",
    "optimizer = optim.Adam(model.parameters(), lr=opt.learning_rate, weight_decay=opt.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if vars(opt).get('start_from', None) is not None:\n",
    "    optimizer.load_state_dict(torch.load(os.path.join(opt.start_from, 'optimizer.pth')))\n",
    " \n",
    "train_loss = []\n",
    "res = []\n",
    "iter_print_count =200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    if update_lr_flag:\n",
    "            # Assign the learning rate\n",
    "        if epoch > opt.learning_rate_decay_start and opt.learning_rate_decay_start >= 0:\n",
    "\n",
    "            frac = (epoch - opt.learning_rate_decay_start) // opt.learning_rate_decay_every\n",
    "            decay_factor = opt.learning_rate_decay_rate  ** frac\n",
    "            opt.current_lr = opt.learning_rate * decay_factor\n",
    "            utils.set_lr(optimizer, opt.current_lr) # set the decayed rate\n",
    "        else:\n",
    "            opt.current_lr = opt.learning_rate\n",
    "            # Assign the scheduled sampling prob\n",
    "            if epoch > opt.scheduled_sampling_start and opt.scheduled_sampling_start >= 0:\n",
    "                frac = (epoch - opt.scheduled_sampling_start) // opt.scheduled_sampling_increase_every\n",
    "                opt.ss_prob = min(opt.scheduled_sampling_increase_prob  * frac, opt.scheduled_sampling_max_prob)\n",
    "                model.ss_prob = opt.ss_prob\n",
    "            update_lr_flag = False \n",
    "\n",
    "    start = time.time()\n",
    "    # Load data from train split (0)\n",
    "    data = loader.get_batch('train')\n",
    "#     print('Read data:', time.time() - start)\n",
    "    torch.cuda.synchronize()  \n",
    "\n",
    "    start = time.time()\n",
    "    tmp = [data['fc_feats'], data['att_feats'], data['labels'], data['masks']]\n",
    "    tmp = [Variable(torch.from_numpy(_), requires_grad=False).cuda() for _ in tmp]\n",
    "    fc_feats, att_feats, labels, masks = tmp \n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "#     print('calculate prediction')\n",
    "    predictions = model(fc_feats,att_feats,labels)  \n",
    "    loss = crit(model(fc_feats, att_feats, labels), labels[:,1:], masks[:,1:])\n",
    " \n",
    "    loss.backward() \n",
    "    \n",
    "    utils.clip_gradient(optimizer, opt.grad_clip)\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.data[0])\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    res.append(loss.data[0])\n",
    "\n",
    "    if iteration % iter_print_count == 0:\n",
    "        print(\"iter {} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                .format(iteration, epoch, sum(train_loss)/len(train_loss), end - start))\n",
    "        train_loss = [] \n",
    "            # Update the iteration and epoch\n",
    "    iteration += 1\n",
    "    if data['bounds']['wrapped']:\n",
    "        epoch += 1\n",
    "        update_lr_flag = True\n",
    "    # Write the training loss summary\n",
    "    if (iteration % opt.losses_log_every == 0):\n",
    "\n",
    "        if tf is not None:\n",
    "            add_summary_value(tf_summary_writer, 'train_loss', loss.data[0], iteration)\n",
    "            add_summary_value(tf_summary_writer, 'learning_rate', opt.current_lr, iteration)\n",
    "            add_summary_value(tf_summary_writer, 'scheduled_sampling_prob', model.ss_prob, iteration)\n",
    "            tf_summary_writer.flush()\n",
    "\n",
    "        loss_history[iteration] = loss.data[0]\n",
    "        lr_history[iteration] = opt.current_lr\n",
    "        ss_prob_history[iteration] = model.ss_prob\n",
    "\n",
    "    # 在验证集上检验效果\n",
    "    if (iteration % opt.save_checkpoint_every == 0):\n",
    "                # eval model\n",
    "            eval_kwargs = {'split': 'val',\n",
    "                            'dataset': opt.input_json}\n",
    "            eval_kwargs.update(vars(opt))\n",
    "            val_loss, predictions, lang_stats = eval_utils.eval_split(model, crit, loader, eval_kwargs)  \n",
    "\n",
    "                # Write validation result into summary\n",
    "            if tf is not None:\n",
    "                add_summary_value(tf_summary_writer, 'validation loss', val_loss, iteration)\n",
    "                for k,v in lang_stats.items():\n",
    "                    add_summary_value(tf_summary_writer, k, v, iteration)\n",
    "                tf_summary_writer.flush()\n",
    "            val_result_history[iteration] = {'loss': val_loss, 'lang_stats': lang_stats, 'predictions': predictions}\n",
    "\n",
    "                # Save model if is improving on validation result\n",
    "            if opt.language_eval == 1:\n",
    "                current_score = lang_stats['CIDEr']\n",
    "            else:\n",
    "                current_score = - val_loss\n",
    "\n",
    "            best_flag = False\n",
    "            if True: # if true\n",
    "                if best_val_score is None or current_score > best_val_score:\n",
    "                    best_val_score = current_score\n",
    "                    best_flag = True\n",
    "                checkpoint_path = os.path.join(opt.checkpoint_path, 'model.pth')\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(\"model saved to {}\".format(checkpoint_path))\n",
    "                optimizer_path = os.path.join(opt.checkpoint_path, 'optimizer.pth')\n",
    "                torch.save(optimizer.state_dict(), optimizer_path)\n",
    "\n",
    "                # Dump miscalleous informations\n",
    "                infos['iter'] = iteration\n",
    "                infos['epoch'] = epoch\n",
    "                infos['iterators'] = loader.iterators\n",
    "                infos['split_ix'] = loader.split_ix\n",
    "                infos['best_val_score'] = best_val_score\n",
    "                infos['opt'] = opt\n",
    "                infos['vocab'] = loader.get_vocab()\n",
    "\n",
    "                histories['val_result_history'] = val_result_history\n",
    "                histories['loss_history'] = loss_history\n",
    "                histories['lr_history'] = lr_history\n",
    "                histories['ss_prob_history'] = ss_prob_history\n",
    "                with open(os.path.join(opt.checkpoint_path, 'infos_'+opt.id+'_attention.pkl'), 'wb') as f:\n",
    "                    cPickle.dump(infos, f)\n",
    "                with open(os.path.join(opt.checkpoint_path, 'histories_'+opt.id+'_attention.pkl'), 'wb') as f:\n",
    "                    cPickle.dump(histories, f)\n",
    "\n",
    "                if best_flag:\n",
    "                    checkpoint_path = os.path.join(opt.checkpoint_path, 'model-best_attention.pth')\n",
    "                    torch.save(model.state_dict(), checkpoint_path)\n",
    "                    print(\"model saved to {}\".format(checkpoint_path))\n",
    "                    with open(os.path.join(opt.checkpoint_path, 'infos_'+opt.id+'best_attention.pkl'), 'wb') as f:\n",
    "                        cPickle.dump(infos, f) \n",
    "    # Stop if reaching max epochs\n",
    "    if epoch >= opt.max_epochs and opt.max_epochs != -1:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}