{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8888/notebooks/prepro_ai_challenger_step2.ipynb#预处理二\" data-toc-modified-id=\"预处理二-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>预处理二</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/prepro_ai_challenger_step2.ipynb#导入工具包，并选择GPU卡\" data-toc-modified-id=\"导入工具包，并选择GPU卡-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>导入工具包，并选择GPU卡</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/prepro_ai_challenger_step2.ipynb#默认参数设置\" data-toc-modified-id=\"默认参数设置-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>默认参数设置</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/prepro_ai_challenger_step2.ipynb#构建词典\" data-toc-modified-id=\"构建词典-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>构建词典</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/prepro_ai_challenger_step2.ipynb#assign_splits\" data-toc-modified-id=\"assign_splits-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>assign_splits</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/prepro_ai_challenger_step2.ipynb#L,-label_start_ix,-label_end_ix,-label_length-=-encode_captions(imgs,-params,-wtoi)\" data-toc-modified-id=\"L,-label_start_ix,-label_end_ix,-label_length-=-encode_captions(imgs,-params,-wtoi)-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>L, label_start_ix, label_end_ix, label_length = encode_captions(imgs, params, wtoi)</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/prepro_ai_challenger_step2.ipynb#使用resnet进行图片特征的提取\" data-toc-modified-id=\"使用resnet进行图片特征的提取-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>使用resnet进行图片特征的提取</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/prepro_ai_challenger_step2.ipynb#其他信息存储\" data-toc-modified-id=\"其他信息存储-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>其他信息存储</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理二\n",
    "这是Image_Caption第二个运行的代码，核心功能是从所有语句中构建字典,并对每个句子进行编码存储；对训练和测试图片采用resnet152进行图片特征的提取."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入工具包，并选择GPU卡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from random import shuffle, seed\n",
    "import string\n",
    "# non-standard dependencies:\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import skimage.io\n",
    "from misc.resnet_utils import myResnet\n",
    "import jieba\n",
    "import misc.resnet as resnet\n",
    "from torchvision import transforms as trn\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 默认参数设置\n",
    "- input_json: 训练集和验证集的coco存储形式的数据的位置.\n",
    "- num_val:验证图片的个数\n",
    "- output_json: 输出数据的位置\n",
    "- output_h5: \n",
    "- word_count_threshold: 词频的阈值,大于等于word_count_threshold的词我们默认选择删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Paras(): \n",
    "    def __init__(self):\n",
    "        self.input_json = './preprocessed_data/coco_ai_challenger_new_version.json'\n",
    "        self.num_val = 30000 \n",
    "        self.output_json = './preprocessed_data/coco_ai_challenger_talk.json'\n",
    "        self.output_h5 = './preprocessed_data/coco_ai_challenger_talk'\n",
    "\n",
    "        self.max_length = 64  \n",
    "        self.images_root = '/home/jiangqy/EXP/competition/data/captions/' \n",
    "        self.word_count_threshold =  2\n",
    "        self.num_test = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = Paras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs = json.load(open(params.input_json, 'r'))\n",
    "imgs = imgs['images']\n",
    "preprocess = trn.Compose([\n",
    "    # trn.ToTensor(),\n",
    "    trn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建词典\n",
    "\n",
    "**vocab,counts = build_vocab(imgs, params):该函数用来构建词典.**\n",
    "\n",
    "- vocab为对应的词表,由输入的所有图像描述包含的词构成.\n",
    "- counts为对应的词频(字典形式),形式为{词:出现次数}.\n",
    "- count_thr用来控制vocab的数量,如果输入的图片中所有caption分词得到的集合中词的个数小于count_thr的,我们一律用UNK来表示.同时我们将每张图片对应的所有词(包含5个描述)进行存储."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(imgs, params):\n",
    "    count_thr = params.word_count_threshold #params.word_count_threshold \n",
    " \n",
    "    counts = {}\n",
    "    for img in imgs:\n",
    "        for sentence in img['sentences']:  \n",
    "            for s in sentence['tokens']:\n",
    "                    for w in s:\n",
    "                        counts[w] = counts.get(w, 0) + 1 \n",
    "\n",
    "    cw = sorted([(count, w) for w, count in counts.iteritems()], reverse=True)\n",
    "    print 'top words and their counts:'\n",
    "    # print '\\n'.join(map(str, cw[:20]))\n",
    "    for i in cw[:20]: \n",
    "        print(i[1])\n",
    "        print(i[0]) \n",
    "\n",
    "        \n",
    "    for i in cw[-20:]: \n",
    "        print(i[1])\n",
    "        print(i[0]) \n",
    "    # print some stats\n",
    "    total_words = sum(counts.itervalues())\n",
    "    total_words\n",
    "    print 'total words:', total_words\n",
    "    print('num of words',len(counts.keys()))\n",
    "    \n",
    "    \n",
    "    bad_words = [w for w, n in counts.iteritems() if n <= count_thr]\n",
    "    vocab = [w for w, n in counts.iteritems() if n > count_thr]\n",
    "    \n",
    "    bad_count = sum(counts[w] for w in bad_words)\n",
    "    print 'number of bad words: %d/%d = %.2f%%' % (len(bad_words), len(counts), len(bad_words) * 100.0 / len(counts))\n",
    "    print 'number of words in vocab would be %d' % (len(vocab),)\n",
    "    print 'number of UNKs: %d/%d = %.2f%%' % (bad_count, total_words, bad_count * 100.0 / total_words)\n",
    "\n",
    "    # lets now produce the final annotations\n",
    "    if bad_count > 0:\n",
    "            # additional special UNK token we will use below to map infrequent words to\n",
    "        print 'inserting the special UNK token'\n",
    "        vocab.append('UNK')\n",
    "\n",
    "    for img in imgs:\n",
    "        img['final_captions'] = []\n",
    "        caption = []\n",
    "        for txt in img['sentences']:\n",
    "            s_ = []\n",
    "            for s in txt['tokens']: \n",
    "                for w in s:\n",
    "                    if counts.get(w, 0) > count_thr:\n",
    "                        s_.append(w)\n",
    "                    else:\n",
    "                        s_.append('UNK')\n",
    "            caption.append(set(s_)) \n",
    "        img['final_captions'].append(caption)\n",
    "\n",
    "    return vocab,counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab,counts = build_vocab(imgs, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- itow: {index: 词},index到词的映射\n",
    "- wtoi: {词 : index}，词到index的映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itow = {i + 1: w for i, w in enumerate(vocab)}  # a 1-indexed vocab translation table\n",
    "wtoi = {w: i + 1 for i, w in enumerate(vocab)}  # inverse table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign_splits\n",
    "该函数就是将图片划分为train和split,标注该样本是训练集中的还是验证集中的样本,其实之前的就已经有了,这边多此一举."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_splits(imgs, params):\n",
    "    num_val = params.num_val \n",
    "    count_val = 0\n",
    "    count_train = 0\n",
    "    \n",
    "    for i,img in enumerate(imgs):\n",
    "        if 'val' in img['filepath']:\n",
    "            img['split'] = 'val'\n",
    "            count_val += 1\n",
    "        else:\n",
    "            count_train += 1\n",
    "            img['split'] = 'train'\n",
    "\n",
    "    print 'assigned %d to train, %d to val.' % (count_train,count_val )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assign_splits(imgs, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L, label_start_ix, label_end_ix, label_length = encode_captions(imgs, params, wtoi)\n",
    "- L:所有语句的编码,如果有1000张图片,每张图片对应5句话,那么L = 1000 $*$ 5, 每一行对应一个句子的编码.\n",
    "- label_start_ix: 每张图片开始的位置句子开始的位置\n",
    "- label_end_ix:每张图片结束的位置句子结束的位置\n",
    "- label_length: 每个句子的长度\n",
    "\n",
    "**<font color=red>label_start_ix[i]到label_end_ix[i]为第i张图片对应的句子部分.其编码为:  L[label_start_ix[i]:label_end_ix[i],:]</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_length = params.max_length\n",
    "N = len(imgs)\n",
    "M = 0\n",
    "#for img in imgs:\n",
    "#    M = M + len(img['final_captions'])\n",
    "M = sum(len(img['final_captions']) for img in imgs)  # total number of captions\n",
    "print('Total number of captions:' + str(N))\n",
    "print('Total number of captions:' + str(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_arrays = []\n",
    "label_start_ix = np.zeros(N, dtype='uint32')  # note: these will be one-indexed\n",
    "label_end_ix = np.zeros(N, dtype='uint32')\n",
    "label_length = np.zeros(M, dtype='uint32')\n",
    "caption_counter = 0\n",
    "counter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_captions(imgs, params, wtoi):\n",
    "    \"\"\"\n",
    "    encode all captions into one large array, which will be 1-indexed.\n",
    "    also produces label_start_ix and label_end_ix which store 1-indexed\n",
    "    and inclusive (Lua-style) pointers to the first and last caption for\n",
    "    each image in the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    max_length = params.max_length\n",
    "    N = len(imgs)\n",
    "    M = 0\n",
    "    #for img in imgs:\n",
    "    #    M = M + len(img['final_captions'])\n",
    "    M = sum(len(img['sentences'][0]['tokens']) for img in imgs)  # total number of captions\n",
    "    print('Total number of images:' + str(N))\n",
    "    print('Total number of captions:' + str(M))\n",
    "    label_arrays = []\n",
    "    label_start_ix = np.zeros(N, dtype='uint32')  # note: these will be one-indexed\n",
    "    label_end_ix = np.zeros(N, dtype='uint32')\n",
    "    label_length = np.zeros(M, dtype='uint32')\n",
    "    caption_counter = 0\n",
    "    counter = 1\n",
    "    for i, img in enumerate(imgs):\n",
    "        n = len(img['sentences'][0])\n",
    "        assert n > 0, 'error: some image has no captions'\n",
    "\n",
    "        Li = np.zeros((n, max_length), dtype='uint32')\n",
    "        for j, sentence in enumerate(img['sentences'][0]['tokens']): \n",
    "#             for sentence in s['tokens']: \n",
    "            \n",
    "            label_length[caption_counter] = min(max_length, len(sentence))  # record the length of this sequence\n",
    "            caption_counter += 1\n",
    "\n",
    "            for k, w in enumerate(sentence): \n",
    "                if k < max_length:\n",
    "                    if w not in wtoi.keys():\n",
    "                        Li[j, k] = wtoi['UNK']\n",
    "                    else:\n",
    "                        Li[j, k] = wtoi[w] \n",
    "\n",
    "        # note: word indices are 1-indexed, and captions are padded with zeros\n",
    "        label_arrays.append(Li)\n",
    "        label_start_ix[i] = counter\n",
    "        label_end_ix[i] = counter + n - 1\n",
    "\n",
    "        counter += n\n",
    "\n",
    "    L = np.concatenate(label_arrays, axis=0)  # put all the labels together\n",
    "    assert L.shape[0] == M, 'lengths don\\'t match? that\\'s weird'\n",
    "    assert np.all(label_length > 0), 'error: some caption had no words?'\n",
    "\n",
    "    print 'encoded captions to array of size ', `L.shape`\n",
    "    return L, label_start_ix, label_end_ix, label_length \n",
    "\n",
    "L, label_start_ix, label_end_ix, label_length = encode_captions(imgs, params, wtoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode captions in large arrays, ready to ship to hdf5 file\n",
    "L, label_start_ix, label_end_ix, label_length = encode_captions(imgs, params, wtoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用resnet进行图片特征的提取\n",
    "\n",
    "导入resnet网络,进行图片特征的抽取,f_fc.h5,f_att.h5分别存储了对应图片的图像特征."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_type = 'resnet152'\n",
    "if resnet_type == 'resnet101':\n",
    "    resnet = resnet.resnet101(True)\n",
    "    resnet.load_state_dict(torch.load('/home/jiangqy/PycharmProjects/Image_Caption/resnet/resnet101.pth'))\n",
    "else:\n",
    "    resnet = resnet.resnet152(True)\n",
    "    resnet.load_state_dict(torch.load('/home/jiangqy/PycharmProjects/Image_Caption/resnet/resnet152.pth'))\n",
    "my_resnet = myResnet(resnet)\n",
    "my_resnet.cuda()\n",
    "my_resnet.eval()\n",
    "\n",
    "N = len(imgs)\n",
    "f_lb = h5py.File(params.output_h5 + '_'+ resnet_type +'_label.h5', \"w\")\n",
    "f_lb.create_dataset(\"labels\", dtype='uint32', data=L)\n",
    "f_lb.create_dataset(\"label_start_ix\", dtype='uint32', data=label_start_ix)\n",
    "f_lb.create_dataset(\"label_end_ix\", dtype='uint32', data=label_end_ix)\n",
    "f_lb.create_dataset(\"label_length\", dtype='uint32', data=label_length)\n",
    "f_lb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_fc = h5py.File(params.output_h5 + '_'+ resnet_type +'_fc.h5', \"w\")\n",
    "f_att = h5py.File(params.output_h5 + '_'+ resnet_type +'_att.h5', \"w\") \n",
    "dset_fc = f_fc.create_dataset(\"fc\", (N, 2048), dtype='float32')\n",
    "dset_att = f_att.create_dataset(\"att\", (N, 20, 20, 2048), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, img in enumerate(imgs):\n",
    "        # load the image\n",
    "    I = skimage.io.imread(os.path.join(params.images_root, img['filepath']+'/'+img['filename']))\n",
    "    # handle grayscale input images\n",
    "    if len(I.shape) == 2:\n",
    "        I = I[:, :, np.newaxis]\n",
    "        I = np.concatenate((I, I, I), axis=2) \n",
    "    I = I.astype('float32') / 255.0\n",
    "    I = torch.from_numpy(I.transpose([2, 0, 1])).cuda()\n",
    "    I = Variable(preprocess(I), volatile=True)\n",
    "    print('start:',I.size())\n",
    "    tmp_fc, tmp_att = my_resnet(I)\n",
    "    print('end:',I.size(),tmp_fc.size(),tmp_att.size())\n",
    "    # write to h5\n",
    "    dset_fc[i] = tmp_fc.data.cpu().float().numpy()\n",
    "    dset_att[i] = tmp_att.data.cpu().float().numpy()\n",
    "    if i % 1000 == 0:\n",
    "        print 'processing %d/%d (%.2f%% done)' % (i, N, i * 100.0 / N)\n",
    "f_fc.close()\n",
    "f_att.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他信息存储\n",
    "** out[images] **\n",
    "- 存储是否是train / val 的信息\n",
    "- 存储图片的id以及图片对应的位置\n",
    "\n",
    "** out[ix_to_word] **\n",
    "- 下标到词的转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = {}\n",
    "out['ix_to_word'] = itow  # encode the (1-indexed) vocab\n",
    "out['images'] = []\n",
    "\n",
    "for i, img in enumerate(imgs):\n",
    "    if i% 1000 == 0:\n",
    "        print(i)\n",
    "    jimg = {}\n",
    "    jimg['split'] = img['split']\n",
    "    if 'filepath' in img: jimg['file_path'] = img['filepath']  # copy it over, might need\n",
    "    if 'cocoid' in img: jimg['id'] = img['cocoid']  # copy over & mantain an id, if present (e.g. coco ids, useful)\n",
    "\n",
    "    out['images'].append(jimg)\n",
    "\n",
    "json.dump(out, open(params.output_json, 'w'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}